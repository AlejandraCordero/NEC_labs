{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Selecting and Analyzing the Dataset\n",
    "## Ames Housing Dataset - Complete Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Ames Housing dataset\n",
    "data = pd.read_csv('AmesHousing.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset shape: {data.shape}\")\n",
    "print(f\"Number of samples: {data.shape[0]}\")\n",
    "print(f\"Number of features: {data.shape[1]}\")\n",
    "\n",
    "# Display first rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(data.info())\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_values = data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Feature': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(f\"Total features with missing values: {len(missing_df)}\")\n",
    "print(\"\\nTop 20 features with most missing values:\")\n",
    "print(missing_df.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "missing_df.head(15).plot(x='Feature', y='Missing_Count', kind='barh', color='coral', legend=False)\n",
    "plt.xlabel('Number of Missing Values')\n",
    "plt.title('Top 15 Features with Missing Values')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "missing_df.head(15).plot(x='Feature', y='Percentage', kind='barh', color='skyblue', legend=False)\n",
    "plt.xlabel('Percentage of Missing Values (%)')\n",
    "plt.title('Missing Values Percentage')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_missing_values.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Target Variable Analysis (SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SalePrice statistics\n",
    "print(\"SalePrice Statistics:\")\n",
    "print(data['SalePrice'].describe())\n",
    "\n",
    "# Visualize SalePrice distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(data['SalePrice'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Sale Price ($)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Sale Prices', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axvline(data['SalePrice'].mean(), color='red', linestyle='--', label=f'Mean: ${data[\"SalePrice\"].mean():,.0f}')\n",
    "axes[0, 0].axvline(data['SalePrice'].median(), color='green', linestyle='--', label=f'Median: ${data[\"SalePrice\"].median():,.0f}')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(data['SalePrice'], vert=True)\n",
    "axes[0, 1].set_ylabel('Sale Price ($)', fontsize=12)\n",
    "axes[0, 1].set_title('Sale Price Box Plot', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Log-transformed histogram\n",
    "log_price = np.log1p(data['SalePrice'])\n",
    "axes[1, 0].hist(log_price, bins=50, color='orange', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Log(Sale Price)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title('Log-Transformed Sale Price Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(data['SalePrice'], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot: SalePrice vs Normal Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_saleprice_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check for outliers\n",
    "Q1 = data['SalePrice'].quantile(0.25)\n",
    "Q3 = data['SalePrice'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = data[(data['SalePrice'] < Q1 - 1.5 * IQR) | (data['SalePrice'] > Q3 + 1.5 * IQR)]\n",
    "print(f\"\\nNumber of outliers: {len(outliers)} ({len(outliers)/len(data)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Categorical vs Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = data.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Number of categorical features: {len(categorical_cols)}\")\n",
    "print(f\"Number of numerical features: {len(numerical_cols)}\")\n",
    "\n",
    "print(\"\\nCategorical features:\")\n",
    "for i, col in enumerate(categorical_cols, 1):\n",
    "    unique_count = data[col].nunique()\n",
    "    print(f\"{i:2d}. {col:20s} - {unique_count:3d} unique values\")\n",
    "\n",
    "# Visualize distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "categories = ['Categorical', 'Numerical']\n",
    "counts = [len(categorical_cols), len(numerical_cols)]\n",
    "colors = ['lightcoral', 'lightskyblue']\n",
    "\n",
    "bars = ax.bar(categories, counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Number of Features', fontsize=12)\n",
    "ax.set_title('Feature Types Distribution', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count}',\n",
    "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_feature_types.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for correlation analysis\n",
    "numeric_df = data.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Get correlations with SalePrice\n",
    "corr_with_price = correlation_matrix['SalePrice'].sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 20 features most correlated with SalePrice:\")\n",
    "print(corr_with_price.head(20))\n",
    "\n",
    "print(\"\\nBottom 20 features (negative correlation):\")\n",
    "print(corr_with_price.tail(20))\n",
    "\n",
    "# Visualize top correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Bar plot of correlations\n",
    "top_corr = corr_with_price.drop('SalePrice').head(15)\n",
    "colors = ['green' if x > 0 else 'red' for x in top_corr.values]\n",
    "axes[0].barh(range(len(top_corr)), top_corr.values, color=colors, alpha=0.7)\n",
    "axes[0].set_yticks(range(len(top_corr)))\n",
    "axes[0].set_yticklabels(top_corr.index)\n",
    "axes[0].set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "axes[0].set_title('Top 15 Features Correlated with SalePrice', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Heatmap of top correlations\n",
    "top_features = corr_with_price.drop('SalePrice').head(10).index.tolist() + ['SalePrice']\n",
    "top_corr_matrix = numeric_df[top_features].corr()\n",
    "\n",
    "sns.heatmap(top_corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=axes[1])\n",
    "axes[1].set_title('Correlation Heatmap: Top 10 Features + SalePrice', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Complete Correlation Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full correlation matrix (may be large, so we'll show top 30 features)\n",
    "plt.figure(figsize=(20, 18))\n",
    "\n",
    "# Select top 30 most correlated features with SalePrice\n",
    "top_30_features = corr_with_price.abs().sort_values(ascending=False).head(30).index.tolist()\n",
    "corr_matrix_top30 = numeric_df[top_30_features].corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix_top30, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr_matrix_top30, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True, linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "plt.title('Correlation Matrix: Top 30 Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_full_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Pairplot Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features for pairplot (to keep it manageable)\n",
    "top_features_pairplot = ['SalePrice', 'Overall Qual', 'Gr Liv Area', 'Garage Area', \n",
    "                         'Total Bsmt SF', 'Year Built', '1st Flr SF']\n",
    "\n",
    "print(f\"Creating pairplot for {len(top_features_pairplot)} features...\")\n",
    "print(\"This may take a moment...\\n\")\n",
    "\n",
    "# Create pairplot\n",
    "pairplot_data = data[top_features_pairplot].dropna()\n",
    "\n",
    "g = sns.pairplot(pairplot_data, \n",
    "                 diag_kind='kde',\n",
    "                 plot_kws={'alpha': 0.6, 's': 30, 'edgecolor': 'k', 'linewidth': 0.5},\n",
    "                 diag_kws={'shade': True, 'linewidth': 2})\n",
    "\n",
    "g.fig.suptitle('Pairplot: Most Important Features', y=1.01, fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_pairplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Pairplot created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of top numerical features\n",
    "top_num_features = ['Overall Qual', 'Gr Liv Area', 'Garage Cars', 'Garage Area', \n",
    "                    'Total Bsmt SF', 'Year Built', '1st Flr SF', 'Full Bath']\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_num_features):\n",
    "    if feature in data.columns:\n",
    "        axes[idx].hist(data[feature].dropna(), bins=30, color='steelblue', \n",
    "                      edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_xlabel(feature, fontsize=11)\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=11)\n",
    "        axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset by removing columns with too many missing values\n",
    "df = data.copy()\n",
    "\n",
    "# Drop columns with more than 50% missing values\n",
    "threshold = 0.5\n",
    "missing_ratio = df.isnull().sum() / len(df)\n",
    "cols_to_drop = missing_ratio[missing_ratio > threshold].index.tolist()\n",
    "\n",
    "print(f\"Dropping {len(cols_to_drop)} columns with >50% missing values:\")\n",
    "print(cols_to_drop)\n",
    "\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"\\nDataset shape after dropping columns: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ordinal encoding mappings\n",
    "ordinal_mappings = {\n",
    "    \"Lot Shape\":         [\"IR3\", \"IR2\", \"IR1\", \"Reg\"],\n",
    "    \"Land Slope\":        [\"Sev\", \"Mod\", \"Gtl\"],\n",
    "    \"Exter Qual\":        [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"Exter Cond\":        [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"Bsmt Qual\":         [\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"Bsmt Cond\":         [\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"Bsmt Exposure\":     [\"None\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    \"BsmtFin Type 1\":    [\"None\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"BsmtFin Type 2\":    [\"None\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"Heating QC\":        [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"Kitchen Qual\":      [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"],\n",
    "    \"Functional\":        [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    \"Paved Drive\":       [\"N\", \"P\", \"Y\"],\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "for col, order in ordinal_mappings.items():\n",
    "    if col in df.columns:\n",
    "        if \"None\" in order:\n",
    "            df[col] = df[col].fillna(\"None\")\n",
    "        \n",
    "        oe = OrdinalEncoder(categories=[order], handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        df[[col]] = oe.fit_transform(df[[col]])\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "# One-Hot Encoding for nominal categorical features\n",
    "onehot_features = [\n",
    "    \"MS Zoning\", \"Street\", \"Land Contour\", \"Utilities\", \"Lot Config\",\n",
    "    \"Neighborhood\", \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\",\n",
    "    \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \"Exterior 2nd\", \"Foundation\",\n",
    "    \"Heating\", \"Central Air\", \"Electrical\", \"Sale Type\", \"Sale Condition\"\n",
    "]\n",
    "\n",
    "# Filter features that exist in dataframe\n",
    "onehot_features = [col for col in onehot_features if col in df.columns]\n",
    "\n",
    "print(f\"Applying One-Hot Encoding to {len(onehot_features)} features...\")\n",
    "\n",
    "ohe = OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown='ignore')\n",
    "encoded = ohe.fit_transform(df[onehot_features])\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(onehot_features), index=df.index)\n",
    "\n",
    "df = pd.concat([df.drop(columns=onehot_features), encoded_df], axis=1)\n",
    "\n",
    "print(f\"\\nDataset shape after encoding: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13 Handle Missing Values and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Fill remaining missing values with median for numerical features\n",
    "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numerical_features:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"\\nTotal missing values after cleaning: {df.isnull().sum().sum()}\")\n",
    "print(f\"Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14 Feature Scaling (Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "if 'SalePrice' in df.columns:\n",
    "    X = df.drop(columns=['SalePrice'])\n",
    "    y = df['SalePrice']\n",
    "else:\n",
    "    raise ValueError(\"SalePrice column not found!\")\n",
    "\n",
    "# Identify numerical columns (excluding one-hot encoded)\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remove ordinal encoded columns from scaling (already scaled 0-n)\n",
    "ordinal_cols = [col for col in ordinal_mappings.keys() if col in num_cols]\n",
    "num_cols_to_scale = [col for col in num_cols if col not in ordinal_cols]\n",
    "\n",
    "print(f\"Scaling {len(num_cols_to_scale)} numerical features using RobustScaler...\")\n",
    "\n",
    "# Apply RobustScaler (robust to outliers)\n",
    "scaler = RobustScaler()\n",
    "X[num_cols_to_scale] = scaler.fit_transform(X[num_cols_to_scale])\n",
    "\n",
    "print(\"Scaling complete!\")\n",
    "print(f\"\\nFinal feature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.15 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Data split completed!\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTraining samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.16 Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X and y back for saving\n",
    "df_preprocessed = X.copy()\n",
    "df_preprocessed['SalePrice'] = y\n",
    "\n",
    "# Save preprocessed data\n",
    "df_preprocessed.to_csv(\"AmesHousing_preprocessing_enhanced.csv\", index=False)\n",
    "print(\"Preprocessed data saved to 'AmesHousing_preprocessing_enhanced.csv'\")\n",
    "\n",
    "# Save train and test sets separately\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "train_df.to_csv(\"AmesHousing_train_enhanced.csv\", index=False)\n",
    "test_df.to_csv(\"AmesHousing_test_enhanced.csv\", index=False)\n",
    "\n",
    "print(\"Training data saved to 'AmesHousing_train_enhanced.csv'\")\n",
    "print(\"Test data saved to 'AmesHousing_test_enhanced.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.17 Baseline Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BASELINE MODEL 1: LINEAR REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr_train = lr_model.predict(X_train)\n",
    "y_pred_lr_test = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse_lr = mean_squared_error(y_train, y_pred_lr_train)\n",
    "train_rmse_lr = np.sqrt(train_mse_lr)\n",
    "train_mae_lr = mean_absolute_error(y_train, y_pred_lr_train)\n",
    "train_r2_lr = r2_score(y_train, y_pred_lr_train)\n",
    "\n",
    "test_mse_lr = mean_squared_error(y_test, y_pred_lr_test)\n",
    "test_rmse_lr = np.sqrt(test_mse_lr)\n",
    "test_mae_lr = mean_absolute_error(y_test, y_pred_lr_test)\n",
    "test_r2_lr = r2_score(y_test, y_pred_lr_test)\n",
    "\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  MSE:  {train_mse_lr:,.2f}\")\n",
    "print(f\"  RMSE: {train_rmse_lr:,.2f}\")\n",
    "print(f\"  MAE:  {train_mae_lr:,.2f}\")\n",
    "print(f\"  R²:   {train_r2_lr:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  MSE:  {test_mse_lr:,.2f}\")\n",
    "print(f\"  RMSE: {test_rmse_lr:,.2f}\")\n",
    "print(f\"  MAE:  {test_mae_lr:,.2f}\")\n",
    "print(f\"  R²:   {test_r2_lr:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_pred_lr_train, alpha=0.6, color='blue', s=30)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Real Values', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[0].set_title(f'Linear Regression - Training Set\\nR² = {train_r2_lr:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test, y_pred_lr_test, alpha=0.6, color='green', s=30)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Real Values', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[1].set_title(f'Linear Regression - Test Set\\nR² = {test_r2_lr:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_linear_regression_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.18 Baseline Model 2: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BASELINE MODEL 2: XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_model.fit(X_train, y_train, \n",
    "              eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "              verbose=False)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb_train = xgb_model.predict(X_train)\n",
    "y_pred_xgb_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse_xgb = mean_squared_error(y_train, y_pred_xgb_train)\n",
    "train_rmse_xgb = np.sqrt(train_mse_xgb)\n",
    "train_mae_xgb = mean_absolute_error(y_train, y_pred_xgb_train)\n",
    "train_r2_xgb = r2_score(y_train, y_pred_xgb_train)\n",
    "\n",
    "test_mse_xgb = mean_squared_error(y_test, y_pred_xgb_test)\n",
    "test_rmse_xgb = np.sqrt(test_mse_xgb)\n",
    "test_mae_xgb = mean_absolute_error(y_test, y_pred_xgb_test)\n",
    "test_r2_xgb = r2_score(y_test, y_pred_xgb_test)\n",
    "\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  MSE:  {train_mse_xgb:,.2f}\")\n",
    "print(f\"  RMSE: {train_rmse_xgb:,.2f}\")\n",
    "print(f\"  MAE:  {train_mae_xgb:,.2f}\")\n",
    "print(f\"  R²:   {train_r2_xgb:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"  MSE:  {test_mse_xgb:,.2f}\")\n",
    "print(f\"  RMSE: {test_rmse_xgb:,.2f}\")\n",
    "print(f\"  MAE:  {test_mae_xgb:,.2f}\")\n",
    "print(f\"  R²:   {test_r2_xgb:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_pred_xgb_train, alpha=0.6, color='purple', s=30)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Real Values', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[0].set_title(f'XGBoost - Training Set\\nR² = {train_r2_xgb:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test, y_pred_xgb_test, alpha=0.6, color='orange', s=30)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Real Values', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[1].set_title(f'XGBoost - Test Set\\nR² = {test_r2_xgb:.4f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_xgboost_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.19 Feature Importance from XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features (XGBoost):\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'].values, color='teal', alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'].values)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.title('Top 20 Feature Importances (XGBoost)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_xgboost_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.20 Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Linear Regression',\n",
    "        'Train_MSE': train_mse_lr,\n",
    "        'Train_RMSE': train_rmse_lr,\n",
    "        'Train_R2': train_r2_lr,\n",
    "        'Test_MSE': test_mse_lr,\n",
    "        'Test_RMSE': test_rmse_lr,\n",
    "        'Test_R2': test_r2_lr\n",
    "    },\n",
    "    {\n",
    "        'Model': 'XGBoost',\n",
    "        'Train_MSE': train_mse_xgb,\n",
    "        'Train_RMSE': train_rmse_xgb,\n",
    "        'Train_R2': train_r2_xgb,\n",
    "        'Test_MSE': test_mse_xgb,\n",
    "        'Test_RMSE': test_rmse_xgb,\n",
    "        'Test_R2': test_r2_xgb\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BASELINE MODELS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('part1_baseline_comparison.csv', index=False)\n",
    "print(\"\\nComparison saved to 'part1_baseline_comparison.csv'\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RMSE comparison\n",
    "models = comparison_df['Model'].values\n",
    "train_rmse = comparison_df['Train_RMSE'].values\n",
    "test_rmse = comparison_df['Test_RMSE'].values\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, train_rmse, width, label='Train RMSE', color='skyblue', alpha=0.8)\n",
    "axes[0].bar(x + width/2, test_rmse, width, label='Test RMSE', color='lightcoral', alpha=0.8)\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_title('RMSE Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "train_r2 = comparison_df['Train_R2'].values\n",
    "test_r2 = comparison_df['Test_R2'].values\n",
    "\n",
    "axes[1].bar(x - width/2, train_r2, width, label='Train R²', color='lightgreen', alpha=0.8)\n",
    "axes[1].bar(x + width/2, test_r2, width, label='Test R²', color='gold', alpha=0.8)\n",
    "axes[1].set_ylabel('R² Score', fontsize=12)\n",
    "axes[1].set_title('R² Score Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('part1_baseline_comparison_chart.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Dataset Overview\n",
    "- **Source**: Ames Housing Dataset\n",
    "- **Total Samples**: 2930\n",
    "- **Original Features**: 82\n",
    "- **After Preprocessing**: Varies based on encoding\n",
    "- **Target Variable**: SalePrice (continuous)\n",
    "\n",
    "### Data Preprocessing Steps\n",
    "1. ✅ Missing value analysis and handling\n",
    "2. ✅ Removal of columns with >50% missing values\n",
    "3. ✅ Ordinal encoding for ordered categorical variables\n",
    "4. ✅ One-hot encoding for nominal categorical variables\n",
    "5. ✅ Feature scaling using RobustScaler\n",
    "6. ✅ Train-test split (80-20)\n",
    "\n",
    "### Visualizations Created\n",
    "1. ✅ Missing values analysis\n",
    "2. ✅ Target variable distribution\n",
    "3. ✅ Feature types distribution\n",
    "4. ✅ Correlation analysis\n",
    "5. ✅ Correlation matrix heatmap\n",
    "6. ✅ Pairplot of top features\n",
    "7. ✅ Feature distributions\n",
    "8. ✅ Model prediction scatter plots\n",
    "9. ✅ Feature importance (XGBoost)\n",
    "10. ✅ Model performance comparison\n",
    "\n",
    "### Baseline Models Performance\n",
    "- **Linear Regression**: Provides interpretable baseline\n",
    "- **XGBoost**: Captures non-linear relationships, better performance\n",
    "\n",
    "### Next Steps\n",
    "- Implement Neural Network (Part 2)\n",
    "- Compare all models (Part 3)\n",
    "- Hyperparameter tuning\n",
    "- Cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
